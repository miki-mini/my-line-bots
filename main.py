import os
import uvicorn
from fastapi import FastAPI, Request, BackgroundTasks, Depends
from core.auth_handler import get_current_username
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# Line Bot Imports
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging import Configuration, ApiClient, MessagingApi

# Vertex AI Imports (Required for Mole)
import vertexai
from vertexai.generative_models import (
    GenerativeModel,
    SafetySetting,
    HarmCategory,
    HarmBlockThreshold
)

# Animal Handler Import (All Animals)
from animals.mole import register_mole_handler
from animals.fox import register_fox_handler
from animals.frog import register_frog_handler
from animals.penguin import register_penguin_handler
from animals.voidoll import register_voidoll_handler
from animals.capybara import register_capybara_handler
from animals.whale import register_whale_handler, get_whale_reply_content # Imported new function
from animals.beaver import register_beaver_handler
from animals.bat import register_bat_handler
from animals.owl import register_owl_handler

from routers import web_apps
from animals import beaver, fox, bat, mole, frog, capybara, penguin, owl

# Google Cloud Imports
from google.cloud import storage
from google.cloud import firestore

# ---------------------------------------------------------------------------
# GLOBAL VARIABLES
# ---------------------------------------------------------------------------
# GLOBAL VARIABLES
# ---------------------------------------------------------------------------
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

app.include_router(web_apps.router)
app.include_router(beaver.router)
app.include_router(fox.router)
app.include_router(bat.router)
app.include_router(mole.router)
app.include_router(frog.router)

app.include_router(capybara.router)
app.include_router(penguin.router)
app.include_router(owl.router)
db = None
storage_client = None
text_model = None
search_model = None  # Added for Owl/Capybara/Fox RAG
startup_errors = []

# ---------------------------------------------------------------------------
# AUTHENTICATION (Basic Auth)
# ---------------------------------------------------------------------------
# Authentication logic moved to core/security.py

# ---------------------------------------------------------------------------
# SEARCH MODEL WRAPPER (Restored for Owl/Search features)
# ---------------------------------------------------------------------------
class SearchModelWrapper:
    """å‹•ç‰©ãŸã¡ãŒä»Šã¾ã§é€šã‚Šä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼"""

    def __init__(self, project_id, location="asia-northeast1"):
        from google import genai
        from google.genai import types

        self.client = genai.Client(
            vertexai=True,
            project=project_id,
            location=location
        )
        self.model_name = "gemini-2.5-flash"
        self.types = types

    def generate_content(self, prompt, generation_config=None):
        """ä»Šã¾ã§é€šã‚Šã®å‘¼ã³å‡ºã—æ–¹ã§ä½¿ãˆã‚‹"""
        from google.genai import types

        config_dict = {
            "tools": [types.Tool(google_search=types.GoogleSearch())]
        }

        if generation_config:
            if "temperature" in generation_config:
                config_dict["temperature"] = generation_config["temperature"]
            if "max_output_tokens" in generation_config:
                config_dict["max_output_tokens"] = generation_config["max_output_tokens"]

        config = types.GenerateContentConfig(**config_dict)

        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=config
        )
        return response

load_dotenv()

# Webhook Handler (Main)
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
handler = WebhookHandler(LINE_CHANNEL_SECRET) if LINE_CHANNEL_SECRET else None
configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN) if LINE_CHANNEL_ACCESS_TOKEN else None

# ğŸ¦¡ Mole (Train) Bot Settings
TRAIN_ACCESS_TOKEN = os.getenv("TRAIN_ACCESS_TOKEN")
TRAIN_CHANNEL_SECRET = os.getenv("TRAIN_CHANNEL_SECRET")
configuration_train = Configuration(access_token=TRAIN_ACCESS_TOKEN) if TRAIN_ACCESS_TOKEN else None
handler_train = WebhookHandler(TRAIN_CHANNEL_SECRET) if TRAIN_CHANNEL_SECRET else None

# ğŸ¦Š Fox (YouTube) Bot Settings
FOX_ACCESS_TOKEN = os.getenv("FOX_ACCESS_TOKEN")
FOX_CHANNEL_SECRET = os.getenv("FOX_CHANNEL_SECRET")
configuration_fox = Configuration(access_token=FOX_ACCESS_TOKEN) if FOX_ACCESS_TOKEN else None
handler_fox = WebhookHandler(FOX_CHANNEL_SECRET) if FOX_CHANNEL_SECRET else None

# ğŸ¸ Frog (Weather) Bot Settings
FROG_ACCESS_TOKEN = os.getenv("FROG_ACCESS_TOKEN")
FROG_CHANNEL_SECRET = os.getenv("FROG_CHANNEL_SECRET")
configuration_frog = Configuration(access_token=FROG_ACCESS_TOKEN) if FROG_ACCESS_TOKEN else None
handler_frog = WebhookHandler(FROG_CHANNEL_SECRET) if FROG_CHANNEL_SECRET else None

# ğŸ§ Penguin (Concierge) Bot Settings
PENGUIN_ACCESS_TOKEN = os.getenv("PENGUIN_ACCESS_TOKEN")
PENGUIN_CHANNEL_SECRET = os.getenv("PENGUIN_CHANNEL_SECRET")
configuration_penguin = Configuration(access_token=PENGUIN_ACCESS_TOKEN) if PENGUIN_ACCESS_TOKEN else None
handler_penguin = WebhookHandler(PENGUIN_CHANNEL_SECRET) if PENGUIN_CHANNEL_SECRET else None

# ğŸ¤– Voidoll (Voice) Bot Settings
VOIDOLL_ACCESS_TOKEN = os.getenv("VOIDOLL_ACCESS_TOKEN")
VOIDOLL_CHANNEL_SECRET = os.getenv("VOIDOLL_CHANNEL_SECRET")
configuration_voidoll = Configuration(access_token=VOIDOLL_ACCESS_TOKEN) if VOIDOLL_ACCESS_TOKEN else None
handler_voidoll = WebhookHandler(VOIDOLL_CHANNEL_SECRET) if VOIDOLL_CHANNEL_SECRET else None

# ğŸ¹ Capybara (Relax) Bot Settings
CAPYBARA_ACCESS_TOKEN = os.getenv("CAPYBARA_ACCESS_TOKEN")
CAPYBARA_CHANNEL_SECRET = os.getenv("CAPYBARA_CHANNEL_SECRET")
configuration_capybara = Configuration(access_token=CAPYBARA_ACCESS_TOKEN) if CAPYBARA_ACCESS_TOKEN else None
handler_capybara = WebhookHandler(CAPYBARA_CHANNEL_SECRET) if CAPYBARA_CHANNEL_SECRET else None

# ğŸ‹ Whale (Space) Bot Settings
WHALE_ACCESS_TOKEN = os.getenv("WHALE_ACCESS_TOKEN")
WHALE_CHANNEL_SECRET = os.getenv("WHALE_CHANNEL_SECRET")
configuration_whale = Configuration(access_token=WHALE_ACCESS_TOKEN) if WHALE_ACCESS_TOKEN else None
handler_whale = WebhookHandler(WHALE_CHANNEL_SECRET) if WHALE_CHANNEL_SECRET else None

# ğŸ¦« Beaver (OCR) Bot Settings
BEAVER_ACCESS_TOKEN = os.getenv("BEAVER_ACCESS_TOKEN")
BEAVER_CHANNEL_SECRET = os.getenv("BEAVER_CHANNEL_SECRET")
configuration_beaver = Configuration(access_token=BEAVER_ACCESS_TOKEN) if BEAVER_ACCESS_TOKEN else None
handler_beaver = WebhookHandler(BEAVER_CHANNEL_SECRET) if BEAVER_CHANNEL_SECRET else None

# ğŸ¦‡ Bat (TV) Bot Settings
BAT_ACCESS_TOKEN = os.getenv("BAT_ACCESS_TOKEN")
BAT_CHANNEL_SECRET = os.getenv("BAT_CHANNEL_SECRET")
configuration_bat = Configuration(access_token=BAT_ACCESS_TOKEN) if BAT_ACCESS_TOKEN else None
handler_bat = WebhookHandler(BAT_CHANNEL_SECRET) if BAT_CHANNEL_SECRET else None

# ğŸ¦‰ Owl (Diet/Search) Bot Settings
# Owl uses HTTP endpoints mostly, but may have LINE settings for future use
OWL_ACCESS_TOKEN = os.getenv("OWL_ACCESS_TOKEN")
OWL_CHANNEL_SECRET = os.getenv("OWL_CHANNEL_SECRET")
# Backup didn't show Owl config vars, but we can define them for safety or skip.
# Based on register_owl_handler(app), it doesn't take config args.

from routers import web_apps



# ğŸ° Rabbit (Moon) Bot Settings
RABBIT_ACCESS_TOKEN = os.getenv("RABBIT_ACCESS_TOKEN")
RABBIT_CHANNEL_SECRET = os.getenv("RABBIT_CHANNEL_SECRET")
configuration_rabbit = Configuration(access_token=RABBIT_ACCESS_TOKEN) if RABBIT_ACCESS_TOKEN else None
handler_rabbit = WebhookHandler(RABBIT_CHANNEL_SECRET) if RABBIT_CHANNEL_SECRET else None

# Helper for debugging
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
# Include Routers (Available at startup)
# app.include_router(web_apps.router) is already included at the top

# Static Files Mount (Ensure images/CSS are served)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.on_event("startup")
def startup_event():
    global text_model, db, storage_client, GCS_BUCKET_NAME, search_model
    try:
        print("ğŸš€ PHASE 1 RESTORATION STARTUP...", flush=True)

        # Initialize Google Cloud Resources
        if GCP_PROJECT_ID:
            # 1. Vertex AI
            vertexai.init(project=GCP_PROJECT_ID, location="asia-northeast1")

            safety_config = [
                SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
                SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
                SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=HarmBlockThreshold.BLOCK_NONE),
                SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            ]

            text_model = GenerativeModel("gemini-2.5-flash", safety_settings=safety_config)
            print("âœ… Gemini (text_model) Initialized!", flush=True)

            # Initialize Search Model (The Owl) via Wrapper
            try:
                search_model = SearchModelWrapper(project_id=GCP_PROJECT_ID)
                print("âœ… Compass (Search Model) Initialized!", flush=True)
            except Exception as sm_err:
                print(f"âš ï¸ Search Model Init Failed: {sm_err}", flush=True)
                search_model = None

            # 2. Firestore & Storage
            db = firestore.Client(project=GCP_PROJECT_ID)
            storage_client = storage.Client(project=GCP_PROJECT_ID)
            GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
            print("âœ… Firestore & Storage Initialized!", flush=True)

            # ---------------------------
            # Register Animal Handlers
            # ---------------------------

            # 1. Mole (4 args)
            register_mole_handler(app, handler_train, configuration_train, text_model)
            print("ğŸ¦¡ Mole Registered!", flush=True)

            # 2. Fox (5 args) - Now passing search_model!
            register_fox_handler(app, handler_fox, configuration_fox, search_model, text_model)
            print("ğŸ¦Š Fox Registered!", flush=True)

            # 3. Frog (5 args) - Now passing search_model!
            register_frog_handler(app, handler_frog, configuration_frog, search_model, text_model)
            print("ğŸ¸ Frog Registered!", flush=True)

            # 4. Penguin (4 args)
            register_penguin_handler(app, handler_penguin, configuration_penguin, text_model)
            print("ğŸ§ Penguin Registered!", flush=True)

            # 7. Owl (ãƒ•ã‚¯ãƒ­ã‚¦) - RouteråŒ–æ¸ˆã¿ã®ãŸã‚é–¢æ•°å‘¼ã³å‡ºã—ä¸è¦ (Deprecated)
            # register_owl_handler(app, None)

            # 8. Voidoll (ãƒœã‚¤ãƒ‰ãƒ¼ãƒ«)
            register_voidoll_handler(app, handler_voidoll, text_model)
            print(" Voidoll Registered!", flush=True)

            # 6. Capybara (5 args: app, handler, config, search_model, text_model)
            register_capybara_handler(app, handler_capybara, configuration_capybara, search_model, text_model)
            print(" Capybara Registered!", flush=True)

            # 7. Whale (4 args: app, handler, config, model) -> Passing text_model
            register_whale_handler(app, handler_whale, configuration_whale, text_model)
            print("ğŸ‹ Whale Registered!", flush=True)

            # 8. Beaver (5 args: app, handler, config, db, text_model)
            register_beaver_handler(app, handler_beaver, configuration_beaver, db, text_model)
            print("ğŸ¦« Beaver Registered!", flush=True)

            # 9. Bat (5 args: app, handler, config, search_model, db)
            register_bat_handler(app, handler_bat, configuration_bat, search_model, db)
            print("ğŸ¦‡ Bat Registered!", flush=True)





        else:
            print("âš ï¸ GCP_PROJECT_ID not set, skipping AI init", flush=True)

    except Exception as e:
        import traceback
        print(f"âŒ Startup Error: {e}", flush=True)
        traceback.print_exc()  # â† è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º

@app.post("/callback")
async def callback(request: Request):
    signature = request.headers["x-line-signature"]
    body = await request.body()
    body_str = body.decode("utf-8")

    try:
        if handler:
            handler.handle(body_str, signature)
    except InvalidSignatureError:
        return HTMLResponse(content="Invalid signature", status_code=400)

    return "OK"

# ---------------------------------------------------------------------------
# API Models
# ---------------------------------------------------------------------------
class WhaleChatRequest(BaseModel):
    text: str

@app.post("/api/whale/chat")
async def chat_whale(request: WhaleChatRequest):
    """
    æ˜Ÿãã˜ã‚‰ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹API

    - **text**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    - **return**: æ˜Ÿãã˜ã‚‰ã‹ã‚‰ã®è¿”ä¿¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒURLãªã©ï¼‰
    """
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®text_modelã‚’ä½¿ç”¨
    # text_modelã¯startupæ™‚ã«åˆæœŸåŒ–ã•ã‚Œã‚‹
    response = get_whale_reply_content(request.text, text_model)
    return {"results": response}

# Endpoints (Web Apps moved to routers/web_apps.py)
# Authentication logic moved to core/security.py

# Index route is handled by web_apps.py

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)

# ---------------------------------------------------------------------------
# ğŸ¦™ ã‚¢ãƒ«ãƒ‘ã‚«ã®ã¾ã¤ã‚¨ã‚¯ã‚µãƒ­ãƒ³ç”¨ API Models & Endpoints
# ---------------------------------------------------------------------------
# ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ main.py ã®ä»¥ä¸‹ã®ä½ç½®ã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼š
# - class WhaleChatRequest(BaseModel): ã®ä¸‹
# - @app.post("/api/whale/chat") ã®ä¸Šã¾ãŸã¯ä¸‹

from pydantic import BaseModel
import base64
import json
import re
from PIL import Image
import io

class EyeAnalysisRequest(BaseModel):
    image: str  # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ

@app.post("/api/alpaca-salon/analyze-eye")
async def analyze_eye(request: EyeAnalysisRequest):
    """
    ğŸ¦™ ã‚¢ãƒ«ãƒ‘ã‚«ã®ã¾ã¤ã‚¨ã‚¯ã‚µãƒ­ãƒ³ - AIç›®åˆ†æAPI

    Gemini 2.5 Flash ã§ç›®ã®å½¢çŠ¶ã‚’åˆ†æã—ã€æœ€é©ãªã¾ã¤ã‚¨ã‚¯ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ææ¡ˆ

    - **image**: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸé¡”å†™çœŸ
    - **return**: ç›®ã®ç‰¹å¾´ã¨æ¨å¥¨ã‚¹ã‚¿ã‚¤ãƒ«
    """
    try:
        # Base64ç”»åƒã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        # data:image/png;base64, ã‚’é™¤å»
        image_data = request.image.split(',')[1] if ',' in request.image else request.image
        image_bytes = base64.b64decode(image_data)

        # Vertex AIã§ç”»åƒåˆ†æ
        from vertexai.generative_models import Part

        prompt = """ã‚ãªãŸã¯ç¾å®¹ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚ã“ã®é¡”å†™çœŸã®ç›®ã‚’åˆ†æã—ã¦ã€æœ€é©ãªã¾ã¤ã’ã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã€Œã®ã¿ã€ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ãªã©ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼‰ï¼š

{
  "eyeShape": "almond",
  "eyeSlant": "upturned",
  "eyelidType": "double",
  "eyeWidth": "medium",
  "recommendations": {
    "volume": 60,
    "curl": "C",
    "length": 1.0,
    "reasoning": "ç›®ã®å½¢çŠ¶ã«åˆã‚ã›ãŸæ¨å¥¨ã‚¹ã‚¿ã‚¤ãƒ«ã§ã™"
  }
}

ã€åˆ†é¡ãƒ«ãƒ¼ãƒ«ã€‘
- eyeShape: "almond" (ã‚¢ãƒ¼ãƒ¢ãƒ³ãƒ‰å‹) / "round" (ä¸¸å‹) / "hooded" (ãƒ•ãƒ¼ãƒ‰å‹)
- eyeSlant: "upturned" (ä¸ŠãŒã‚Šç›®) / "downturned" (ä¸‹ãŒã‚Šç›®) / "straight" (å¹³è¡Œ)
- eyelidType: "monolid" (ä¸€é‡) / "double" (äºŒé‡)
- eyeWidth: "narrow" (ç‹­ã‚) / "medium" (æ¨™æº–) / "wide" (åºƒã‚)
- volume: 40, 60, 90, 120 ã®ã„ãšã‚Œã‹
- curl: "J", "C", "D" ã®ã„ãšã‚Œã‹
- length: 0.8, 1.0, 1.2 ã®ã„ãšã‚Œã‹
- reasoning: æ—¥æœ¬èªã§1-2æ–‡ã®ç°¡æ½”ãªç†ç”±

JSONä»¥å¤–ã®æ–‡å­—ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

        # ç”»åƒãƒ‘ãƒ¼ãƒ„ã‚’ä½œæˆ
        image_part = Part.from_data(image_bytes, mime_type="image/png")

        # Gemini 2.5 Flash ã§åˆ†æ
        response = text_model.generate_content(
            [prompt, image_part],
            generation_config={
                "temperature": 0.4,
                "max_output_tokens": 1024,
            }
        )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡º
        response_text = response.text
        print(f"ğŸ“ Gemini Response: {response_text[:500]}...", flush=True)  # ãƒ‡ãƒãƒƒã‚°ç”¨

        # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
        json_str = None

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ```json ... ```
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            print("âœ… JSON found in code block", flush=True)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ``` ... ``` (jsonãªã—)
        if not json_str:
            json_match = re.search(r'```\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                print("âœ… JSON found in generic code block", flush=True)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: { ... } ç›´æ¥
        if not json_str:
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
                print("âœ… JSON found as raw object", flush=True)

        # ãƒ‘ãƒ¼ã‚¹å‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if json_str:
            json_str = json_str.strip()
            # ä¸è¦ãªæ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
            json_str = re.sub(r'\s+', ' ', json_str)
        else:
            print(f"âŒ No JSON found in response", flush=True)
            raise ValueError(f"JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:200]}")

        try:
            analysis = json.loads(json_str)
            print(f"âœ… JSON parsed successfully: {analysis}", flush=True)
        except json.JSONDecodeError as je:
            print(f"âŒ JSON Parse Error: {je}", flush=True)
            print(f"ğŸ“„ Raw JSON String: {json_str[:300]}", flush=True)
            raise ValueError(f"JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {str(je)}")

        return {
            "success": True,
            "analysis": analysis
        }

    except Exception as e:
        import traceback
        print(f"âŒ Eye Analysis Error: {e}", flush=True)
        traceback.print_exc()

        # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨æ–‡ã‚’è¿”ã™
        error_details = str(e)
        if 'response_text' in locals():
            error_details += f"\n\nã€Geminiã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‘\n{response_text[:1000]}"

        return {
            "success": False,
            "error": error_details,
            "analysis": None
        }

# ---------------------------------------------------------------------------
# ğŸ¦‹ Butterfly (Checko) ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚«ãƒ©ãƒ¼ãƒ»éª¨æ ¼è¨ºæ–­ API
# ---------------------------------------------------------------------------
class ButterflyDiagnosisRequest(BaseModel):
    image: str  # Base64 encoded image
    mode: str = "color"  # "color" or "skeleton" (skeleton uses MediaPipe mostly, but maybe we want AI opinion too?)
    lighting: str = "sun"  # "sun", "office", "bulb"

@app.post("/api/butterfly/diagnose")
async def diagnose_butterfly(request: ButterflyDiagnosisRequest):
    """
    ğŸ¦‹ Butterfly (Checko) - AIãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚«ãƒ©ãƒ¼è¨ºæ–­API
    """
    try:
        # 1. Setup Model (Use standard 1.5-flash)
        from vertexai.generative_models import GenerativeModel, SafetySetting, HarmCategory, HarmBlockThreshold

        safety_config = [
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
        ]

        # Using a fresh instance to ensure settings
        bf_model = GenerativeModel("gemini-2.5-flash", safety_settings=safety_config)

        # 2. Decode Image
        image_data = request.image.split(',')[1] if ',' in request.image else request.image
        image_bytes = base64.b64decode(image_data)

        from vertexai.generative_models import Part
        image_part = Part.from_data(image_bytes, mime_type="image/png")

        # 3. Build Prompt (Softened)
        lighting_text = ""
        if request.lighting == "office":
            lighting_text = "æ’®å½±ç’°å¢ƒã¯è›å…‰ç¯ï¼ˆé’ç™½ã„å…‰ï¼‰ã®ä¸‹ã§ã™ã€‚"
        elif request.lighting == "bulb":
            lighting_text = "æ’®å½±ç’°å¢ƒã¯é›»çƒï¼ˆã‚ªãƒ¬ãƒ³ã‚¸è‰²ã®å…‰ï¼‰ã®ä¸‹ã§ã™ã€‚"
        else:
            lighting_text = "æ’®å½±ç’°å¢ƒã¯è‡ªç„¶å…‰ã®æƒ³å®šã§ã™ã€‚"

        prompt = f"""
ã‚ãªãŸã¯ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ã“ã®å†™çœŸã®äººç‰©ã«ä¼¼åˆã†ã€Œã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆï¼ˆãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚«ãƒ©ãƒ¼ï¼‰ã€ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€å…¨èº«ãŒå†™ã£ã¦ã„ã‚‹å ´åˆã¯ã€ã‚¹ã‚¿ã‚¤ãƒ«ãŒã‚ˆãè¦‹ãˆã‚‹ã€Œæœè£…ã®ã‚·ãƒ«ã‚¨ãƒƒãƒˆï¼ˆéª¨æ ¼ã‚¿ã‚¤ãƒ—ï¼‰ã€ã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚

{lighting_text}
â€»åŒ»ç™‚è¡Œç‚ºã‚„æ–­å®šçš„ãªè¨ºæ–­ã§ã¯ãªãã€ã‚ãã¾ã§ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã®æ¥½ã—ã¿ã¨ã—ã¦ã®ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

{{
  "personalColor": {{
    "season": "Autumn",
    "base": "Yellow Base",
    "characteristics": "è½ã¡ç€ã„ãŸãƒãƒƒãƒˆãªè‚Œè³ª...",
    "bestColors": ["Terracotta", "Mustard", "Khaki"],
    "lightingCorrectionNote": "ç…§æ˜ã®è‰²å‘³ã‚’è€ƒæ…®ã—ã€è£œæ­£ã—ã¦åˆ¤æ–­ã—ã¾ã—ãŸã€‚"
  }},
  "skeletonType": {{
    "type": "Straight",
    "description": "ã‚·ãƒ³ãƒ—ãƒ«ã§ãƒãƒªã®ã‚ã‚‹ç´ æãŒãŠã™ã™ã‚..."
  }},
  "faceType": {{
    "shape": "Oval",
    "impression": "Elegant"
  }}
}}

ã€é¸æŠè‚¢ã€‘
season: Spring, Summer, Autumn, Winter
base: Yellow Base, Blue Base
skeletonType.type: Straight, Wave, Natural, null

JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

        # 4. Generate
        response = bf_model.generate_content(
            [prompt, image_part],
            generation_config={
                "temperature": 0.4,
                "max_output_tokens": 1024,
            }
        )

        # 5. Parse
        response_text = response.text
        print(f"ğŸ¦‹ Butterfly Response: {response_text[:200]}...", flush=True)

        # JSON Extraction
        json_str = None
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        elif re.search(r'\{[\s\S]*\}', response_text):
            json_str = re.search(r'\{[\s\S]*\}', response_text).group(0)

        if not json_str:
            # Fallback for safety block without exception
            return {
                "success": False,
                "error": "AIã‹ã‚‰ã®å¿œç­”ãŒèª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ(Safety Filterãªã©)ã€‚åˆ¥ã®å†™çœŸã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"
            }

        json_str = re.sub(r'//.*', '', json_str)
        diagnosis = json.loads(json_str)

        return {
            "success": True,
            "result": diagnosis
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        }
