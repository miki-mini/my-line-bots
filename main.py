import os
# Matplotlibã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’/tmpã«è¨­å®š (Cloud Runå¯¾ç­–)
os.environ['MPLCONFIGDIR'] = '/tmp'
import uvicorn
from fastapi import FastAPI, Request, BackgroundTasks, Depends
from core.auth_handler import get_current_username
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# Line Bot Imports
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging import Configuration, ApiClient, MessagingApi

# Vertex AI Imports (Required for Mole)
import vertexai
from vertexai.generative_models import (
    GenerativeModel,
    SafetySetting,
    HarmCategory,
    HarmBlockThreshold,
    Part
)

# Animal Handler Import (All Animals)
from animals.mole import register_mole_handler
from animals.fox import register_fox_handler
from animals.frog import register_frog_handler
from animals.penguin import register_penguin_handler
from animals.voidoll import register_voidoll_handler
from animals.capybara import register_capybara_handler
from animals.whale import register_whale_handler, get_whale_reply_content # Imported new function
from animals.beaver import register_beaver_handler
from animals.bat import register_bat_handler
from animals.owl import register_owl_handler

from routers import web_apps
from animals import beaver, fox, bat, mole, frog, capybara, penguin, owl

# Google Cloud Imports
from google.cloud import storage
from google.cloud import firestore

# ---------------------------------------------------------------------------
# GLOBAL VARIABLES
# ---------------------------------------------------------------------------
# GLOBAL VARIABLES
# ---------------------------------------------------------------------------
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

app.include_router(web_apps.router)
app.include_router(beaver.router)
app.include_router(fox.router)
app.include_router(bat.router)
app.include_router(mole.router)
app.include_router(frog.router)

app.include_router(capybara.router)
app.include_router(penguin.router)
app.include_router(owl.router)
db = None
storage_client = None
text_model = None
search_model = None  # Added for Owl/Capybara/Fox RAG
startup_errors = []

# ---------------------------------------------------------------------------
# AUTHENTICATION (Basic Auth)
# ---------------------------------------------------------------------------
# Authentication logic moved to core/security.py

# ---------------------------------------------------------------------------
# SEARCH MODEL WRAPPER (Restored for Owl/Search features)
# ---------------------------------------------------------------------------
class SearchModelWrapper:
    """å‹•ç‰©ãŸã¡ãŒä»Šã¾ã§é€šã‚Šä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼"""

    def __init__(self, project_id, location="asia-northeast1"):
        from google import genai
        from google.genai import types

        self.client = genai.Client(
            vertexai=True,
            project=project_id,
            location=location
        )
        self.model_name = "gemini-2.5-flash"
        self.types = types

    def generate_content(self, prompt, generation_config=None):
        """ä»Šã¾ã§é€šã‚Šã®å‘¼ã³å‡ºã—æ–¹ã§ä½¿ãˆã‚‹"""
        from google.genai import types

        config_dict = {
            "tools": [types.Tool(google_search=types.GoogleSearch())]
        }

        if generation_config:
            if "temperature" in generation_config:
                config_dict["temperature"] = generation_config["temperature"]
            if "max_output_tokens" in generation_config:
                config_dict["max_output_tokens"] = generation_config["max_output_tokens"]

        config = types.GenerateContentConfig(**config_dict)

        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=config
        )
        return response

load_dotenv()

# Webhook Handler (Main)
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
handler = WebhookHandler(LINE_CHANNEL_SECRET) if LINE_CHANNEL_SECRET else None
configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN) if LINE_CHANNEL_ACCESS_TOKEN else None

# ğŸ¦¡ Mole (Train) Bot Settings
TRAIN_ACCESS_TOKEN = os.getenv("TRAIN_ACCESS_TOKEN")
TRAIN_CHANNEL_SECRET = os.getenv("TRAIN_CHANNEL_SECRET")
configuration_train = Configuration(access_token=TRAIN_ACCESS_TOKEN) if TRAIN_ACCESS_TOKEN else None
handler_train = WebhookHandler(TRAIN_CHANNEL_SECRET) if TRAIN_CHANNEL_SECRET else None

# ğŸ¦Š Fox (YouTube) Bot Settings
FOX_ACCESS_TOKEN = os.getenv("FOX_ACCESS_TOKEN")
FOX_CHANNEL_SECRET = os.getenv("FOX_CHANNEL_SECRET")
configuration_fox = Configuration(access_token=FOX_ACCESS_TOKEN) if FOX_ACCESS_TOKEN else None
handler_fox = WebhookHandler(FOX_CHANNEL_SECRET) if FOX_CHANNEL_SECRET else None

# ğŸ¸ Frog (Weather) Bot Settings
FROG_ACCESS_TOKEN = os.getenv("FROG_ACCESS_TOKEN")
FROG_CHANNEL_SECRET = os.getenv("FROG_CHANNEL_SECRET")
configuration_frog = Configuration(access_token=FROG_ACCESS_TOKEN) if FROG_ACCESS_TOKEN else None
handler_frog = WebhookHandler(FROG_CHANNEL_SECRET) if FROG_CHANNEL_SECRET else None

# ğŸ§ Penguin (Concierge) Bot Settings
PENGUIN_ACCESS_TOKEN = os.getenv("PENGUIN_ACCESS_TOKEN")
PENGUIN_CHANNEL_SECRET = os.getenv("PENGUIN_CHANNEL_SECRET")
configuration_penguin = Configuration(access_token=PENGUIN_ACCESS_TOKEN) if PENGUIN_ACCESS_TOKEN else None
handler_penguin = WebhookHandler(PENGUIN_CHANNEL_SECRET) if PENGUIN_CHANNEL_SECRET else None

# ğŸ¤– Voidoll (Voice) Bot Settings
VOIDOLL_ACCESS_TOKEN = os.getenv("VOIDOLL_ACCESS_TOKEN")
VOIDOLL_CHANNEL_SECRET = os.getenv("VOIDOLL_CHANNEL_SECRET")
configuration_voidoll = Configuration(access_token=VOIDOLL_ACCESS_TOKEN) if VOIDOLL_ACCESS_TOKEN else None
handler_voidoll = WebhookHandler(VOIDOLL_CHANNEL_SECRET) if VOIDOLL_CHANNEL_SECRET else None

# ğŸ¹ Capybara (Relax) Bot Settings
CAPYBARA_ACCESS_TOKEN = os.getenv("CAPYBARA_ACCESS_TOKEN")
CAPYBARA_CHANNEL_SECRET = os.getenv("CAPYBARA_CHANNEL_SECRET")
configuration_capybara = Configuration(access_token=CAPYBARA_ACCESS_TOKEN) if CAPYBARA_ACCESS_TOKEN else None
handler_capybara = WebhookHandler(CAPYBARA_CHANNEL_SECRET) if CAPYBARA_CHANNEL_SECRET else None

# ğŸ‹ Whale (Space) Bot Settings
WHALE_ACCESS_TOKEN = os.getenv("WHALE_ACCESS_TOKEN")
WHALE_CHANNEL_SECRET = os.getenv("WHALE_CHANNEL_SECRET")
configuration_whale = Configuration(access_token=WHALE_ACCESS_TOKEN) if WHALE_ACCESS_TOKEN else None
handler_whale = WebhookHandler(WHALE_CHANNEL_SECRET) if WHALE_CHANNEL_SECRET else None

# ğŸ¦« Beaver (OCR) Bot Settings
BEAVER_ACCESS_TOKEN = os.getenv("BEAVER_ACCESS_TOKEN")
BEAVER_CHANNEL_SECRET = os.getenv("BEAVER_CHANNEL_SECRET")
configuration_beaver = Configuration(access_token=BEAVER_ACCESS_TOKEN) if BEAVER_ACCESS_TOKEN else None
handler_beaver = WebhookHandler(BEAVER_CHANNEL_SECRET) if BEAVER_CHANNEL_SECRET else None

# ğŸ¦‡ Bat (TV) Bot Settings
BAT_ACCESS_TOKEN = os.getenv("BAT_ACCESS_TOKEN")
BAT_CHANNEL_SECRET = os.getenv("BAT_CHANNEL_SECRET")
configuration_bat = Configuration(access_token=BAT_ACCESS_TOKEN) if BAT_ACCESS_TOKEN else None
handler_bat = WebhookHandler(BAT_CHANNEL_SECRET) if BAT_CHANNEL_SECRET else None

# ğŸ¦‰ Owl (Diet/Search) Bot Settings
# Owl uses HTTP endpoints mostly, but may have LINE settings for future use
OWL_ACCESS_TOKEN = os.getenv("OWL_ACCESS_TOKEN")
OWL_CHANNEL_SECRET = os.getenv("OWL_CHANNEL_SECRET")
# Backup didn't show Owl config vars, but we can define them for safety or skip.
# Based on register_owl_handler(app), it doesn't take config args.

from routers import web_apps



# ğŸ° Rabbit (Moon) Bot Settings
RABBIT_ACCESS_TOKEN = os.getenv("RABBIT_ACCESS_TOKEN")
RABBIT_CHANNEL_SECRET = os.getenv("RABBIT_CHANNEL_SECRET")
configuration_rabbit = Configuration(access_token=RABBIT_ACCESS_TOKEN) if RABBIT_ACCESS_TOKEN else None
handler_rabbit = WebhookHandler(RABBIT_CHANNEL_SECRET) if RABBIT_CHANNEL_SECRET else None

# Helper for debugging
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
# Include Routers (Available at startup)
# app.include_router(web_apps.router) is already included at the top

# Static Files Mount (Ensure images/CSS are served)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.on_event("startup")
def startup_event():
    global text_model, db, storage_client, GCS_BUCKET_NAME, search_model
    try:
        print("ğŸš€ PHASE 1 RESTORATION STARTUP...", flush=True)

        # Initialize Google Cloud Resources
        if GCP_PROJECT_ID:
            # 1. Vertex AI
            vertexai.init(project=GCP_PROJECT_ID, location="us-central1")

            safety_config = [
                SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
                SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
                SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=HarmBlockThreshold.BLOCK_NONE),
                SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            ]

            text_model = GenerativeModel("gemini-2.5-flash", safety_settings=safety_config)
            print("âœ… Gemini (text_model) Initialized!", flush=True)

            # Initialize Search Model (The Owl) via Wrapper
            try:
                search_model = SearchModelWrapper(project_id=GCP_PROJECT_ID)
                print("âœ… Compass (Search Model) Initialized!", flush=True)
            except Exception as sm_err:
                print(f"âš ï¸ Search Model Init Failed: {sm_err}", flush=True)
                search_model = None

            # 2. Firestore & Storage
            db = firestore.Client(project=GCP_PROJECT_ID)
            storage_client = storage.Client(project=GCP_PROJECT_ID)
            GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
            print("âœ… Firestore & Storage Initialized!", flush=True)

            # ---------------------------
            # Register Animal Handlers
            # ---------------------------

            # 1. Mole (4 args)
            register_mole_handler(app, handler_train, configuration_train, text_model)
            print("ğŸ¦¡ Mole Registered!", flush=True)

            # 2. Fox (5 args) - Now passing search_model!
            register_fox_handler(app, handler_fox, configuration_fox, search_model, text_model)
            print("ğŸ¦Š Fox Registered!", flush=True)

            # 3. Frog (5 args) - Now passing search_model!
            register_frog_handler(app, handler_frog, configuration_frog, search_model, text_model)
            print("ğŸ¸ Frog Registered!", flush=True)

            # 4. Penguin (4 args)
            register_penguin_handler(app, handler_penguin, configuration_penguin, text_model)
            print("ğŸ§ Penguin Registered!", flush=True)

            # 7. Owl (ãƒ•ã‚¯ãƒ­ã‚¦) - RouteråŒ–æ¸ˆã¿ã®ãŸã‚é–¢æ•°å‘¼ã³å‡ºã—ä¸è¦ (Deprecated)
            # register_owl_handler(app, None)

            # 8. Voidoll (ãƒœã‚¤ãƒ‰ãƒ¼ãƒ«)
            register_voidoll_handler(app, handler_voidoll, text_model)
            print(" Voidoll Registered!", flush=True)

            # 6. Capybara (5 args: app, handler, config, search_model, text_model)
            register_capybara_handler(app, handler_capybara, configuration_capybara, search_model, text_model)
            print(" Capybara Registered!", flush=True)

            # 7. Whale (4 args: app, handler, config, model) -> Passing text_model
            register_whale_handler(app, handler_whale, configuration_whale, text_model)
            print("ğŸ‹ Whale Registered!", flush=True)

            # 8. Beaver (5 args: app, handler, config, db, text_model)
            register_beaver_handler(app, handler_beaver, configuration_beaver, db, text_model)
            print("ğŸ¦« Beaver Registered!", flush=True)

            # 9. Bat (5 args: app, handler, config, search_model, db)
            register_bat_handler(app, handler_bat, configuration_bat, search_model, db)
            print("ğŸ¦‡ Bat Registered!", flush=True)





        else:
            print("âš ï¸ GCP_PROJECT_ID not set, skipping AI init", flush=True)

    except Exception as e:
        import traceback
        print(f"âŒ Startup Error: {e}", flush=True)
        traceback.print_exc()  # â† è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º

@app.post("/callback")
async def callback(request: Request):
    signature = request.headers["x-line-signature"]
    body = await request.body()
    body_str = body.decode("utf-8")

    try:
        if handler:
            handler.handle(body_str, signature)
    except InvalidSignatureError:
        return HTMLResponse(content="Invalid signature", status_code=400)

    return "OK"

# ---------------------------------------------------------------------------
# API Models
# ---------------------------------------------------------------------------
class WhaleChatRequest(BaseModel):
    text: str

@app.post("/api/whale/chat")
async def chat_whale(request: WhaleChatRequest):
    """
    æ˜Ÿãã˜ã‚‰ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹API

    - **text**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    - **return**: æ˜Ÿãã˜ã‚‰ã‹ã‚‰ã®è¿”ä¿¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒURLãªã©ï¼‰
    """
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®text_modelã‚’ä½¿ç”¨
    # text_modelã¯startupæ™‚ã«åˆæœŸåŒ–ã•ã‚Œã‚‹
    response = get_whale_reply_content(request.text, text_model)
    return {"results": response}

# Endpoints (Web Apps moved to routers/web_apps.py)
# Authentication logic moved to core/security.py

# Index route is handled by web_apps.py

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)

# ---------------------------------------------------------------------------
# ğŸ¦™ ã‚¢ãƒ«ãƒ‘ã‚«ã®ã¾ã¤ã‚¨ã‚¯ã‚µãƒ­ãƒ³ç”¨ API Models & Endpoints
# ---------------------------------------------------------------------------
# ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ main.py ã®ä»¥ä¸‹ã®ä½ç½®ã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼š
# - class WhaleChatRequest(BaseModel): ã®ä¸‹
# - @app.post("/api/whale/chat") ã®ä¸Šã¾ãŸã¯ä¸‹

from pydantic import BaseModel
import base64
import json
import re
from PIL import Image
import io

class EyeAnalysisRequest(BaseModel):
    image: str  # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ

@app.post("/api/alpaca-salon/analyze-eye")
async def analyze_eye(request: EyeAnalysisRequest):
    """
    ğŸ¦™ ã‚¢ãƒ«ãƒ‘ã‚«ã®ã¾ã¤ã‚¨ã‚¯ã‚µãƒ­ãƒ³ - AIç›®åˆ†æAPI

    Gemini 2.5 Flash ã§ç›®ã®å½¢çŠ¶ã‚’åˆ†æã—ã€æœ€é©ãªã¾ã¤ã‚¨ã‚¯ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ææ¡ˆ

    - **image**: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸé¡”å†™çœŸ
    - **return**: ç›®ã®ç‰¹å¾´ã¨æ¨å¥¨ã‚¹ã‚¿ã‚¤ãƒ«
    """
    try:
        # Base64ç”»åƒã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        # data:image/png;base64, ã‚’é™¤å»
        image_data = request.image.split(',')[1] if ',' in request.image else request.image
        image_bytes = base64.b64decode(image_data)

        # Vertex AIã§ç”»åƒåˆ†æ
        from vertexai.generative_models import Part

        prompt = """ã‚ãªãŸã¯ç¾å®¹ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚ã“ã®é¡”å†™çœŸã®ç›®ã‚’åˆ†æã—ã¦ã€æœ€é©ãªã¾ã¤ã’ã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã€Œã®ã¿ã€ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ãªã©ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼‰ï¼š

{
  "eyeShape": "almond",
  "eyeSlant": "upturned",
  "eyelidType": "double",
  "eyeWidth": "medium",
  "recommendations": {
    "volume": 60,
    "curl": "C",
    "length": 1.0,
    "reasoning": "ç›®ã®å½¢çŠ¶ã«åˆã‚ã›ãŸæ¨å¥¨ã‚¹ã‚¿ã‚¤ãƒ«ã§ã™"
  }
}

ã€åˆ†é¡ãƒ«ãƒ¼ãƒ«ã€‘
- eyeShape: "almond" (ã‚¢ãƒ¼ãƒ¢ãƒ³ãƒ‰å‹) / "round" (ä¸¸å‹) / "hooded" (ãƒ•ãƒ¼ãƒ‰å‹)
- eyeSlant: "upturned" (ä¸ŠãŒã‚Šç›®) / "downturned" (ä¸‹ãŒã‚Šç›®) / "straight" (å¹³è¡Œ)
- eyelidType: "monolid" (ä¸€é‡) / "double" (äºŒé‡)
- eyeWidth: "narrow" (ç‹­ã‚) / "medium" (æ¨™æº–) / "wide" (åºƒã‚)
- volume: 40, 60, 90, 120 ã®ã„ãšã‚Œã‹
- curl: "J", "C", "D" ã®ã„ãšã‚Œã‹
- length: 0.8, 1.0, 1.2 ã®ã„ãšã‚Œã‹
- reasoning: æ—¥æœ¬èªã§1-2æ–‡ã®ç°¡æ½”ãªç†ç”±

JSONä»¥å¤–ã®æ–‡å­—ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

        # ç”»åƒãƒ‘ãƒ¼ãƒ„ã‚’ä½œæˆ
        image_part = Part.from_data(image_bytes, mime_type="image/png")

        # Gemini 2.5 Flash ã§åˆ†æ
        response = text_model.generate_content(
            [prompt, image_part],
            generation_config={
                "temperature": 0.4,
                "max_output_tokens": 1024,
            }
        )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡º
        response_text = response.text
        print(f"ğŸ“ Gemini Response: {response_text[:500]}...", flush=True)  # ãƒ‡ãƒãƒƒã‚°ç”¨

        # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
        json_str = None

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ```json ... ```
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            print("âœ… JSON found in code block", flush=True)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ``` ... ``` (jsonãªã—)
        if not json_str:
            json_match = re.search(r'```\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                print("âœ… JSON found in generic code block", flush=True)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: { ... } ç›´æ¥
        if not json_str:
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
                print("âœ… JSON found as raw object", flush=True)

        # ãƒ‘ãƒ¼ã‚¹å‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if json_str:
            json_str = json_str.strip()
            # ä¸è¦ãªæ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
            json_str = re.sub(r'\s+', ' ', json_str)
        else:
            print(f"âŒ No JSON found in response", flush=True)
            raise ValueError(f"JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:200]}")

        try:
            analysis = json.loads(json_str)
            print(f"âœ… JSON parsed successfully: {analysis}", flush=True)
        except json.JSONDecodeError as je:
            print(f"âŒ JSON Parse Error: {je}", flush=True)
            print(f"ğŸ“„ Raw JSON String: {json_str[:300]}", flush=True)
            raise ValueError(f"JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {str(je)}")

        return {
            "success": True,
            "analysis": analysis
        }

    except Exception as e:
        import traceback
        print(f"âŒ Eye Analysis Error: {e}", flush=True)
        traceback.print_exc()

        # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨æ–‡ã‚’è¿”ã™
        error_details = str(e)
        if 'response_text' in locals():
            error_details += f"\n\nã€Geminiã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‘\n{response_text[:1000]}"

        return {
            "success": False,
            "error": error_details,
            "analysis": None
        }

# ---------------------------------------------------------------------------
# ğŸ¦‹ Butterfly (Checko) ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚«ãƒ©ãƒ¼ãƒ»éª¨æ ¼è¨ºæ–­ API
# ---------------------------------------------------------------------------
class ButterflyDiagnosisRequest(BaseModel):
    image: str  # Base64 encoded image
    mode: str = "color"  # "color" or "skeleton" (skeleton uses MediaPipe mostly, but maybe we want AI opinion too?)
    lighting: str = "sun"  # "sun", "office", "bulb"

@app.post("/api/butterfly/diagnose")
async def diagnose_butterfly(request: ButterflyDiagnosisRequest):
    """
    ğŸ¦‹ Butterfly (Checko) - AIãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚«ãƒ©ãƒ¼è¨ºæ–­API
    """
    try:
        # 1. Setup Model (Use standard 1.5-flash)
        from vertexai.generative_models import GenerativeModel, SafetySetting, HarmCategory, HarmBlockThreshold

        safety_config = [
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
        ]

        # Using a fresh instance to ensure settings
        # Using a fresh instance to ensure settings

        bf_model = GenerativeModel("gemini-2.5-flash", safety_settings=safety_config)

        # 2. Decode Image
        image_data = request.image.split(',')[1] if ',' in request.image else request.image
        image_bytes = base64.b64decode(image_data)

        from vertexai.generative_models import Part
        image_part = Part.from_data(image_bytes, mime_type="image/png")

        # 3. Build Prompt (Softened)
        lighting_text = ""
        if request.lighting == "office":
            lighting_text = "æ’®å½±ç’°å¢ƒã¯è›å…‰ç¯ï¼ˆé’ç™½ã„å…‰ï¼‰ã®ä¸‹ã§ã™ã€‚"
        elif request.lighting == "bulb":
            lighting_text = "æ’®å½±ç’°å¢ƒã¯é›»çƒï¼ˆã‚ªãƒ¬ãƒ³ã‚¸è‰²ã®å…‰ï¼‰ã®ä¸‹ã§ã™ã€‚"
        else:
            lighting_text = "æ’®å½±ç’°å¢ƒã¯è‡ªç„¶å…‰ã®æƒ³å®šã§ã™ã€‚"

        prompt = f"""
ã‚ãªãŸã¯ãƒ˜ã‚¢ãƒ¡ã‚¤ã‚¯ã¨ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ã“ã®å†™çœŸã®äººç‰©ã®é­…åŠ›ã‚’å¼•ãå‡ºã™ãŸã‚ã®è¨ºæ–­ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚

1. **ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚«ãƒ©ãƒ¼**: è‚Œã‚„ç³ã®è‰²ã‹ã‚‰ä¼¼åˆã†è‰²ï¼ˆã‚·ãƒ¼ã‚ºãƒ³ï¼‰ã‚’åˆ†æã€‚ç…§æ˜ã®è‰²è¢«ã‚ŠãŒã‚ã‚‹å ´åˆã¯è£œæ­£ã—ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
2. **é¡”ã‚¿ã‚¤ãƒ—è¨ºæ–­**: é¡”ã®è¼ªéƒ­ï¼ˆä¸¸é¡”ã€é¢é•·ã€ãƒ™ãƒ¼ã‚¹å‹ãªã©ï¼‰ã¨ç‰¹å¾´ã‚’åˆ†æã€‚
3. **ä¼¼åˆã†é«ªå‹**: é¡”ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ãŸãŠã™ã™ã‚ã®ãƒ˜ã‚¢ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ææ¡ˆã€‚
4. **éª¨æ ¼ã‚¿ã‚¤ãƒ—**: ã‚‚ã—å…¨èº«ãŒå†™ã£ã¦ã„ã‚‹å ´åˆã¯éª¨æ ¼è¨ºæ–­ã‚‚è¡Œã†ï¼ˆå†™ã£ã¦ã„ãªã„å ´åˆã¯ã€Œåˆ¤å®šä¸èƒ½ã€ã¨è¨˜è¼‰ï¼‰ã€‚

{lighting_text}
â€»åŒ»ç™‚åˆ¤æ–­ã§ã¯ãªãã€ç¾å®¹ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰ã€‘
{{
  "personalColor": {{
    "season": "Autumn",
    "base": "Yellow Base",
    "characteristics": "è‚Œã¯é™¶å™¨ã®ã‚ˆã†ãªãƒãƒƒãƒˆãªè³ªæ„Ÿã§...",
    "bestColors": ["Terracotta", "Mustard", "Khaki"],
    "lightingCorrectionNote": "é›»çƒè‰²ã®å½±éŸ¿ã‚’è€ƒæ…®ã—ã€é»„ã¿ã‚’å¼•ã„ã¦åˆ¤æ–­ã—ã¾ã—ãŸã€‚"
  }},
  "faceType": {{
    "shape": "Round",
    "description": "è¦ªã—ã¿ã‚„ã™ã„å°è±¡ã®ä¸¸é¡”ã‚¿ã‚¤ãƒ—...",
    "bestHairstyles": ["ã‚·ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒãƒ³ã‚°", "ã²ã—å½¢ãƒœãƒ–", "é¡”å‘¨ã‚Šã«ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å…¥ã‚ŒãŸãƒ­ãƒ³ã‚°"]
  }},
  "skeletonType": {{
    "type": "Straight",
    "description": "ä¸ŠåŠèº«ã«åšã¿ãŒã‚ã‚‹ã‚¿ã‚¤ãƒ—ã€‚vãƒãƒƒã‚¯ãªã©ãŒä¼¼åˆã„ã¾ã™..."
  }}
}}

ã€é¸æŠè‚¢ã€‘
- season: Spring, Summer, Autumn, Winter
- base: Yellow Base, Blue Base
- faceType.shape: Round (ä¸¸é¡”), Oval (åµå‹), Long (é¢é•·), Base (ãƒ™ãƒ¼ã‚¹å‹), Triangle (é€†ä¸‰è§’)
- skeletonType.type: Straight, Wave, Natural, null

JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

        # 4. Generate
        response = bf_model.generate_content(
            [prompt, image_part],
            generation_config={
                "temperature": 0.4,
                "max_output_tokens": 4096,
            }
        )

        # 5. Parse
        print(f"ğŸ¦‹ Finish Reason: {response.candidates[0].finish_reason if response.candidates else 'NO CANDIDATES'}", flush=True)
        try:
            response_text = response.text
        except ValueError:
            return {
                "success": False,
                "error": "AIãŒå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆå®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç­‰ãŒåŸå› ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚"
            }
        print(f"ğŸ¦‹ Butterfly Response: {response_text[:200]}...", flush=True)

        # JSON Extraction
        json_str = None
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        elif re.search(r'\{[\s\S]*\}', response_text):
            json_str = re.search(r'\{[\s\S]*\}', response_text).group(0)

        if not json_str:
            # Fallback for safety block without exception
            return {
                "success": False,
                "error": "AIã‹ã‚‰ã®å¿œç­”ãŒèª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ(Safety Filterãªã©)ã€‚åˆ¥ã®å†™çœŸã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"
            }

        try:
            json_str = re.sub(r'//.*', '', json_str) # Remove comments
            diagnosis = json.loads(json_str)
            return {
                "success": True,
                "result": diagnosis
            }
        except json.JSONDecodeError as e:
            print(f"âŒ Error generating content: {e}")
            # Fallback
            result_json = {
                "faceType": {
                    "shape": "Analysis Failed",
                    "description": "AIã«ã‚ˆã‚‹è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„ã€‚",
                    "bestHairstyles": ["N/A"]
                },
                "personalColor": {
                    "season": "Unknown",
                    "base": "Unknown",
                    "characteristics": "è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                    "bestColors": [],
                    "lightingCorrectionNote": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                }
            }

            return {"success": True, "result": result_json}

    except Exception as e:
        print(f"âŒ System Error: {e}")
        return {"success": False, "error": str(e)}


# ==========================================
# ğŸ¦© Flamingo Skeleton Diagnosis Endpoint
# ==========================================
class FlamingoDiagnosisRequest(BaseModel):
    image: str  # Base64 encoded image

@app.post("/api/flamingo/diagnose")
async def diagnose_flamingo(request: FlamingoDiagnosisRequest):
    try:
        # 1. Image Processing
        image_data = request.image.split(',')[1] if ',' in request.image else request.image
        image_bytes = base64.b64decode(image_data)
        image_part = Part.from_data(data=image_bytes, mime_type="image/png")

        # 2. Prompt Engineering
        prompt = """
        ã‚ãªãŸã¯ãƒ—ãƒ­ã®éª¨æ ¼è¨ºæ–­å£«ï¼ˆãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã‚¢ãƒŠãƒªã‚¹ãƒˆï¼‰ã§ã™ã€‚
        é€ã‚‰ã‚ŒãŸå…¨èº«å†™çœŸï¼ˆã¾ãŸã¯ä¸ŠåŠèº«å†™çœŸï¼‰ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éª¨æ ¼ã‚¿ã‚¤ãƒ—ã‚’è¨ºæ–­ã—ã¦ãã ã•ã„ã€‚

        ã€è¨ºæ–­ã‚¿ã‚¤ãƒ—ã€‘ä»¥ä¸‹ã®3ã¤ã‹ã‚‰æœ€ã‚‚è¿‘ã„1ã¤ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
        - Straight (ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ): ç­‹è‚‰ãŒã¤ãã‚„ã™ãã€èº«ä½“ã«åšã¿ãŒã‚ã‚‹ã€‚ãƒ¡ãƒªãƒãƒªãƒœãƒ‡ã‚£ã€‚
        - Wave (ã‚¦ã‚§ãƒ¼ãƒ–): è¯å¥¢ã§åšã¿ãŒãªãã€ä¸‹é‡å¿ƒã€‚æ›²ç·šçš„ãªãƒ©ã‚¤ãƒ³ã€‚
        - Natural (ãƒŠãƒãƒ¥ãƒ©ãƒ«): éª¨æ ¼ãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦ã€é–¢ç¯€ãŒå¤§ãã„ã€‚ãƒ•ãƒ¬ãƒ¼ãƒ æ„ŸãŒã‚ã‚‹ã€‚

        ã€å‡ºåŠ›å½¢å¼ (JSON)ã€‘
        ```json
        {
            "skeletonType": {
                "type": "Straight" | "Wave" | "Natural",
                "description": "ã‚ãªãŸã®ä½“å‹ã®ç‰¹å¾´ã¨ã€ã“ã®éª¨æ ¼ã‚¿ã‚¤ãƒ—ã®èª¬æ˜ã€‚",
                "advice": "ä¼¼åˆã†ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ï¼ˆç´ æã€ã‚·ãƒ«ã‚¨ãƒƒãƒˆãªã©ï¼‰ã€‚"
            }
        }
        ```
        """

        # 3. Model Generation (Using gemini-2.5-flash for speed)
        # Note: Using existing text_model or bf_model?
        # Let's use a fresh instance to ensure standard settings.
        # Use gemini-2.5-flash for Flamingo as it needs speed/availability.

        safety_config = [
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
        ]

        flamingo_model = GenerativeModel("gemini-2.5-flash", safety_settings=safety_config)

        response = flamingo_model.generate_content(
            [prompt, image_part],
            generation_config={"temperature": 0.4, "max_output_tokens": 1024}
        )

        # 4. JSON Extraction
        txt = response.text
        json_str = txt
        if "```json" in txt:
            json_str = txt.split("```json")[1].split("```")[0].strip()
        elif "```" in txt:
            json_str = txt.split("```")[1].split("```")[0].strip()

        result_json = json.loads(json_str)
        return {"success": True, "result": result_json}

    except Exception as e:
        print(f"âŒ Flamingo Error: {e}")
        return {"success": False, "error": str(e)}
